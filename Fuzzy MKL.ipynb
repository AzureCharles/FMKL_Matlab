{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f80beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFuzzynumber(trainset, delta):\n",
    "    if trainset.size == 0:\n",
    "        print('The input dataset is null!')\n",
    "        return\n",
    "    \n",
    "    group1 = trainset[trainset[:, -1] == -1, :-1]\n",
    "    group2 = trainset[trainset[:, -1] == 1, :-1]\n",
    "    \n",
    "    mean_g1 = torch.mean(group1, dim=0)\n",
    "    mean_g2 = torch.mean(group2, dim=0)\n",
    "    \n",
    "    max_g1 = torch.max(torch.norm(group1 - mean_g1, dim=1))\n",
    "    max_g2 = torch.max(torch.norm(group2 - mean_g2, dim=1))\n",
    "    \n",
    "    fms = torch.zeros(trainset.shape[0], 1)\n",
    "    for i in range(trainset.shape[0]):\n",
    "        if trainset[i, -1] == -1:\n",
    "            fms[i, 0] = 1 - (torch.norm(trainset[i, :-1] - mean_g1) / (max_g1 + delta))\n",
    "        elif trainset[i, -1] == 1:\n",
    "            fms[i, 0] = 1 - (torch.norm(trainset[i, :-1] - mean_g2) / (max_g2 + delta))\n",
    "    \n",
    "    return fms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94488d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def compute_fuzzy_number(trainset, delta):\n",
    "    if trainset.size == 0:\n",
    "        print('The input dataset is null!')\n",
    "        return None\n",
    "    else:\n",
    "        group1 = trainset[trainset[:, -1] == -1, :-1]\n",
    "        group2 = trainset[trainset[:, -1] == 1, :-1]\n",
    "\n",
    "        mean_g1 = torch.mean(group1, dim=0)\n",
    "        mean_g2 = torch.mean(group2, dim=0)\n",
    "\n",
    "        max_g1 = torch.max(torch.norm(group1 - mean_g1, dim=1))\n",
    "        max_g2 = torch.max(torch.norm(group2 - mean_g2, dim=1))\n",
    "\n",
    "        fms = torch.zeros(trainset.shape[0])\n",
    "        for i in range(trainset.shape[0]):\n",
    "            if trainset[i, -1] == -1:\n",
    "                fms[i] = 1 - (torch.sqrt(torch.norm(trainset[i, :-1] - mean_g1)) / (max_g1 + delta))\n",
    "            if trainset[i, -1] == 1:\n",
    "                fms[i] = 1 - (torch.sqrt(torch.norm(trainset[i, :-1] - mean_g2)) / (max_g2 + delta))\n",
    "\n",
    "        return fms\n",
    "\n",
    "\n",
    "def trainFSVC(train, fms, C, kernel_fn, para):\n",
    "    X = train[:, :-1]\n",
    "    Y = train[:, -1]\n",
    "    nrow, ncol = X.shape\n",
    "\n",
    "    Kmatrix = kernel_fn(X, X, para)\n",
    "\n",
    "    H = torch.outer(Y, Y) * Kmatrix\n",
    "\n",
    "    Aeq = torch.tensor(Y).unsqueeze(0)\n",
    "    beq = torch.tensor([0.])\n",
    "\n",
    "    lb = torch.zeros(nrow)\n",
    "    ub = C * fms\n",
    "\n",
    "    a0 = torch.zeros(nrow) + 0.0001\n",
    "\n",
    "    x = torch.zeros(nrow, requires_grad=True)\n",
    "\n",
    "    optimizer = optim.Adadelta([x])\n",
    "\n",
    "    num_iter = 2000\n",
    "    for t in range(num_iter):\n",
    "        loss = torch.sum(torch.matmul(x.unsqueeze(-1), H) * x) / 2 - torch.sum(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        x.data = torch.min(torch.max(x.data, lb), ub)\n",
    "\n",
    "    lamda = x.detach()\n",
    "\n",
    "    epsilon = 1e-8\n",
    "    i_sv = torch.where(torch.abs(lamda) > epsilon)[0]\n",
    "    tmp = kernel_fn(X, X[i_sv, :], para) * (lamda[i_sv] * Y[i_sv])\n",
    "    b = 1 / Y[i_sv] - torch.mean(tmp, dim=0)\n",
    "    boundary = torch.mean(b)\n",
    "\n",
    "    return lamda, boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b5dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def testFSVC(lamda, boundary, train, test, ker, para):\n",
    "    nrow, ncol = train.shape\n",
    "    nrowt, ncolt = test.shape\n",
    "    \n",
    "    X = torch.tensor(train[:, :ncol-1], dtype=torch.float)\n",
    "    Y = torch.tensor(train[:, ncol-1], dtype=torch.float)\n",
    "    \n",
    "    Xt = torch.tensor(test[:, :ncolt-1], dtype=torch.float)\n",
    "    \n",
    "    Kmatrix = kernel(X, Xt, ker, para)\n",
    "    \n",
    "    tmp = torch.mm(Kmatrix, torch.mul(lamda, Y).unsqueeze(1)).squeeze(1)\n",
    "    py = torch.sign(tmp + boundary).numpy()\n",
    "    \n",
    "    predictedY = np.hstack((test[:, ncolt-1].reshape(-1, 1), py.reshape(-1, 1)))\n",
    "    \n",
    "    stat = torch.zeros(18)\n",
    "    for n in range(nrowt):\n",
    "        if predictedY[n, 0] == -1:\n",
    "            stat[0] += 1\n",
    "            if predictedY[n, 1] == -1:\n",
    "                stat[2] += 1\n",
    "            else:\n",
    "                stat[3] += 1\n",
    "        if predictedY[n, 0] == 1:\n",
    "            stat[1] += 1\n",
    "            if predictedY[n, 1] == 1:\n",
    "                stat[4] += 1\n",
    "            else:\n",
    "                stat[5] += 1\n",
    "    \n",
    "    stat[6] = stat[3] / (stat[3] + stat[4])\n",
    "    stat[7] = stat[5] / (stat[5] + stat[2])\n",
    "    stat[8] = (stat[2] + stat[5]) / (stat[2] + stat[3] + stat[4] + stat[5])\n",
    "    stat[9] = (stat[3] + stat[4]) / (stat[2] + stat[3] + stat[4] + stat[5])\n",
    "    stat[10] = (stat[3] + stat[4]) / (stat[2] + stat[3] + stat[4] + stat[5])\n",
    "    stat[11] = stat[4] / (stat[3] + stat[4])\n",
    "    stat[12] = stat[4] / (stat[4] + stat[5])\n",
    "    stat[13] = stat[4] / (stat[4] + stat[5])\n",
    "    stat[14] = stat[3] / (stat[3] + stat[2])\n",
    "    stat[15] = 2 * stat[4] / (2 * stat[4] + stat[3] + stat[5])\n",
    "    stat[16] = (stat[3] * stat[4] - stat[2] * stat[5]) / \\\n",
    "                (torch.sqrt((stat[4] + stat[5]) * (stat[2] + stat[3]) * (stat[3] + stat[5]) * (stat[2] + stat[4])) + 1e-10)\n",
    "\n",
    "    return predictedY, stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class deepMKL(nn.Module):\n",
    "    def __init__(self, input_size, output_size, n_layers=3):\n",
    "        super(deepMKL, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.betas = nn.Parameter(torch.ones(n_layers, 4) / 4)\n",
    "\n",
    "    def forward(self, x, sig):\n",
    "        Kf = []\n",
    "        for i in range(self.n_layers):\n",
    "            beta = self.betas[i]\n",
    "            k = torch.exp(-torch.sum((x.unsqueeze(1) - x.unsqueeze(2)) ** 2, dim=-1) / (2 * sig ** 2))\n",
    "            Kf.append(torch.matmul(k, beta))\n",
    "            x = Kf[-1]\n",
    "        return Kf, torch.matmul(Kf[-1], Kf[-1].t())\n",
    "\n",
    "    def train(self, x, y, lr=1e-4, max_iter=100, c=10):\n",
    "        n_samples = x.shape[0]\n",
    "        dotx = torch.matmul(x, x.t())\n",
    "        sig = self._determine_sig(dotx.numpy())\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        span = 0\n",
    "        for t in range(max_iter):\n",
    "            Kf, Ks = self.forward(x, sig)\n",
    "            model = SVC(C=c, kernel='precomputed')\n",
    "            model.fit(Ks.numpy(), y.numpy())\n",
    "\n",
    "            if self.n_layers == 1:\n",
    "                grad, span_t = self._grad_1_layer(model, Kf[0], y)\n",
    "            elif self.n_layers == 2:\n",
    "                grad, span_t = self._grad_2_layer(model, Kf[0], Kf[1], sig, y)\n",
    "            elif self.n_layers == 3:\n",
    "                grad, span_t = self._grad_3_layer(model, Kf[0], Kf[1], Kf[2], sig, y)\n",
    "\n",
    "            self.betas.data -= lr * grad\n",
    "            self.betas.data[self.betas.data < 0] = 0\n",
    "            if self.betas[-1].sum() > 1:\n",
    "                self.betas[-1] /= self.betas[-1].sum()\n",
    "\n",
    "            if np.isnan(self.betas.numpy().sum()):\n",
    "                raise ValueError('Learning rate is too high')\n",
    "            elif t > 5 and abs(span - span_t) < 1e-4:\n",
    "                break\n",
    "            span = span_t\n",
    "\n",
    "    def _determine_sig(self, dotx):\n",
    "        n = dotx.shape[0]\n",
    "        s = np.median(dotx)\n",
    "        return np.sqrt(s / 2)\n",
    "\n",
    "    def _grad_1_layer(self, model, Kf, y):\n",
    "        K = torch.exp(-torch.sum((Kf.unsqueeze(1) - Kf.unsqueeze(2)) ** 2, dim=-1) / (2 * self.sig ** 2))\n",
    "        y_pred = torch.from_numpy(model.decision_function(K.numpy())).float()\n",
    "        grad = torch.zeros_like(self.betas)\n",
    "        for i in range(self.n_layers):\n",
    "            grad[i, 0] = torch.sum(Kf[i] * (1 - y * y_pred))\n",
    "            grad[i, 1] = torch.sum(Kf[i] * (1 - y_pred ** 2))\n",
    "            grad[i, 2] = torch.sum(Kf[i] * (1 - y * y_pred) * y)\n",
    "            grad[i, 3] = torch.sum(Kf[i] * (1 - y * y_pred) * y ** 2)\n",
    "        span = torch.norm(grad)\n",
    "        return grad, span\n",
    "    def _grad_2_layer(self, model, Kf1, Kf2, sig, y):\n",
    "        K1 = torch.exp(-torch.sum((Kf1.unsqueeze(1) - Kf1.unsqueeze(2)) ** 2, dim=-1) / (2 * sig ** 2))\n",
    "        K2 = torch.exp(-torch.sum((Kf2.unsqueeze(1) - Kf2.unsqueeze(2)) ** 2, dim=-1) / (2 * sig ** 2))\n",
    "        K12 = torch.matmul(Kf1.unsqueeze(-1), Kf2.unsqueeze(-2))\n",
    "        K12 = torch.exp(-torch.sum((K12 - K12.permute(0, 2, 1)) ** 2, dim=-1) / (2 * sig ** 2))\n",
    "\n",
    "        K = self.betas[0, 0] * K1 + self.betas[0, 1] * K2 + self.betas[0, 2] * K12 + self.betas[0, 3] * torch.eye(K1.shape[0])\n",
    "\n",
    "        y_pred = torch.from_numpy(model.decision_function(K.numpy())).float()\n",
    "        grad = torch.zeros_like(self.betas)\n",
    "        for i in range(self.n_layers):\n",
    "            grad[i, 0] = torch.sum(K1 * (1 - y * y_pred))\n",
    "            grad[i, 1] = torch.sum(K2 * (1 - y * y_pred))\n",
    "            grad[i, 2] = torch.sum(K12 * (1 - y * y_pred))\n",
    "            grad[i, 3] = torch.sum(torch.eye(K1.shape[0]) * (1 - y * y_pred))\n",
    "        span = torch.norm(grad)\n",
    "        return grad, span\n",
    "\n",
    "    def _grad_3_layer(self, model, Kf1, Kf2, Kf3, sig, y):\n",
    "        K1 = torch.exp(-torch.sum((Kf1.unsqueeze(1) - Kf1.unsqueeze(2)) ** 2, dim=-1) / (2 * sig ** 2))\n",
    "        K2 = torch.exp(-torch.sum((Kf2.unsqueeze(1) - Kf2.unsqueeze(2)) ** 2, dim=-1) / (2 * sig ** 2))\n",
    "        K3 = torch.exp(-torch.sum((Kf3.unsqueeze(1) - Kf3.unsqueeze(2)) ** 2, dim=-1) / (2 * sig ** 2))\n",
    "        K12 = torch.matmul(Kf1.unsqueeze(-1), Kf2.unsqueeze(-2))\n",
    "        K12 = torch.exp(-torch.sum((K12 - K12.permute(0, 2, 1)) ** 2, dim=-1) / (2 * sig ** 2))\n",
    "        K13 = torch.matmul(Kf1.unsqueeze(-1), Kf3.unsqueeze(-2))\n",
    "        K13 = torch.exp(-torch.sum((K13 - K13.permute(0, 2, 1)) ** 2, dim=-1) / (2 * sig ** 2))\n",
    "        K23 = torch.matmul(Kf2.unsqueeze(-1), Kf3.unsqueeze(-2))\n",
    "        K23 = torch.exp(-torch.sum((K23 - K23.permute(0, 2, 1))** 2, dim=-1) / (2 * sig ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def deepMKL_train(x, y, nLayers, LR=1e-4, maxI=100, C=10):\n",
    "    \"\"\"\n",
    "    Deep Multiple Kernel Learning by Span Bound\n",
    "\n",
    "    Inputs:\n",
    "    (1) x = trainng data matrix, where rows are instances and columns are features\n",
    "    (2) y = training target vector, where rows are instances\n",
    "    (3) nLayers = number of layers, 1, 2 or 3\n",
    "    (4) LR = learning rate (default=1E-4)\n",
    "    (5) maxI = maximum number of iterations (default=100)\n",
    "    (6) C = SVM penalty constant (default=10)\n",
    "\n",
    "    Outputs:\n",
    "    (1) model = LIBSVM model\n",
    "    (2) net = net parameters\n",
    "\n",
    "    Citation: Strobl EV & Visweswaran S. Deep Multiple Kernel Learning.\n",
    "    ICMLA, 2013.\n",
    "    \"\"\"\n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "    r = x.shape[0]\n",
    "\n",
    "    # initialize weights\n",
    "    betas = torch.ones(nLayers, 4) / 4\n",
    "\n",
    "    # initialize kernels\n",
    "    dotx = torch.matmul(x, x.T)\n",
    "    sig = determineSig(dotx)\n",
    "    Kf = computeKernels(dotx, sig, betas, nLayers)\n",
    "\n",
    "    # alternating opt\n",
    "    span = 0\n",
    "    for t in range(maxI):\n",
    "\n",
    "        # train SVM\n",
    "        Ks = Kf[:, nLayers - 1].reshape(r, r)\n",
    "        svc = SVC(kernel='precomputed', C=C)\n",
    "        svc.fit(Ks.numpy(), y.numpy())\n",
    "        model = svc\n",
    "\n",
    "        # kernels\n",
    "        K, Kf = computeKernels(dotx, sig, betas, nLayers)\n",
    "\n",
    "        # span gradient\n",
    "        if nLayers == 1:\n",
    "            betas, spanT = grad1Layer(model, betas, LR, Kf, K, y)\n",
    "        elif nLayers == 2:\n",
    "            betas, spanT = grad2Layer(model, betas, LR, Kf, K, sig, y)\n",
    "        elif nLayers == 3:\n",
    "            betas, spanT = grad3Layer(model, betas, LR, Kf, K, sig, y)\n",
    "\n",
    "        # feasible region projection\n",
    "        betas[betas < 0] = 0  # non-negative\n",
    "        if torch.sum(betas[-1, :]) > 1:\n",
    "            betas[-1, :] = betas[-1, :] / torch.sum(betas[-1, :])  # trace final layer upper bound\n",
    "\n",
    "        # stopping conditions\n",
    "        if np.isnan(np.sum(betas)):\n",
    "            raise ValueError('Learning rate is too high')\n",
    "        elif torch.abs(span - spanT) < 1e-4 and t > 5:\n",
    "            break\n",
    "        span = spanT\n",
    "\n",
    "    # final model\n",
    "    net = {'w': betas, 'sig': sig, 'nLayers': nLayers, 'n': r}\n",
    "\n",
    "    return model, net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7286e6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "tensor([[-0.4686, -1.0732, -1.4533],\n",
      "        [ 0.2133, -1.4165,  0.4282],\n",
      "        [ 0.4139, -2.4091,  0.7029],\n",
      "        [ 0.2786, -0.6810, -1.0052],\n",
      "        [ 1.9398,  0.7333,  0.4589]])\n",
      "y:\n",
      "tensor([[-1.4241,  0.2257,  1.1952],\n",
      "        [ 0.9051,  1.0643, -1.3562],\n",
      "        [-1.8180,  1.1094, -0.3707],\n",
      "        [ 2.1814,  0.9993,  1.5191]])\n",
      "x_unsqueezed:\n",
      "tensor([[[-0.4686, -1.0732, -1.4533]],\n",
      "\n",
      "        [[ 0.2133, -1.4165,  0.4282]],\n",
      "\n",
      "        [[ 0.4139, -2.4091,  0.7029]],\n",
      "\n",
      "        [[ 0.2786, -0.6810, -1.0052]],\n",
      "\n",
      "        [[ 1.9398,  0.7333,  0.4589]]])\n",
      "y_unsqueezed:\n",
      "tensor([[[-1.4241,  0.2257,  1.1952],\n",
      "         [ 0.9051,  1.0643, -1.3562],\n",
      "         [-1.8180,  1.1094, -0.3707],\n",
      "         [ 2.1814,  0.9993,  1.5191]]])\n",
      "diff:\n",
      "tensor([[[ 0.9556, -1.2989, -2.6486],\n",
      "         [-1.3736, -2.1375, -0.0971],\n",
      "         [ 1.3495, -2.1827, -1.0826],\n",
      "         [-2.6500, -2.0726, -2.9724]],\n",
      "\n",
      "        [[ 1.6374, -1.6421, -0.7670],\n",
      "         [-0.6918, -2.4807,  1.7844],\n",
      "         [ 2.0313, -2.5259,  0.7989],\n",
      "         [-1.9681, -2.4158, -1.0909]],\n",
      "\n",
      "        [[ 1.8381, -2.6347, -0.4923],\n",
      "         [-0.4911, -3.4733,  2.0591],\n",
      "         [ 2.2320, -3.5185,  1.0737],\n",
      "         [-1.7675, -3.4084, -0.8161]],\n",
      "\n",
      "        [[ 1.7027, -0.9066, -2.2005],\n",
      "         [-0.6265, -1.7452,  0.3509],\n",
      "         [ 2.0966, -1.7904, -0.6345],\n",
      "         [-1.9028, -1.6803, -2.5243]],\n",
      "\n",
      "        [[ 3.3639,  0.5076, -0.7364],\n",
      "         [ 1.0347, -0.3310,  1.8151],\n",
      "         [ 3.7579, -0.3762,  0.8296],\n",
      "         [-0.2416, -0.2661, -1.0602]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 导入pytorch库\n",
    "import torch\n",
    "\n",
    "# 初始化两个随机的张量，形状分别为(5, 3)和(4, 3)\n",
    "x = torch.randn(5, 3) # x.shape = (5, 3)\n",
    "y = torch.randn(4, 3) # y.shape = (4, 3)\n",
    "\n",
    "# 打印x和y\n",
    "print(\"x:\")\n",
    "print(x)\n",
    "print(\"y:\")\n",
    "print(y)\n",
    "\n",
    "# 对x和y分别使用unsqueeze()，参数分别为1和0\n",
    "x_unsqueezed = x.unsqueeze(1) # x_unsqueezed.shape = (5, 1, 3)\n",
    "y_unsqueezed = y.unsqueeze(0) # y_unsqueezed.shape = (1, 4, 3)\n",
    "\n",
    "# 打印x_unsqueezed和y_unsqueezed\n",
    "print(\"x_unsqueezed:\")\n",
    "print(x_unsqueezed)\n",
    "print(\"y_unsqueezed:\")\n",
    "print(y_unsqueezed)\n",
    "\n",
    "# 对x_unsqueezed和y_unsqueezed进行减法运算，利用广播机制\n",
    "diff = x_unsqueezed - y_unsqueezed # diff.shape = (5, 4, 3)\n",
    "\n",
    "# 打印diff\n",
    "print(\"diff:\")\n",
    "print(diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0dbc3af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_categorical' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_46136\\1435689161.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mYh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mXtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXva\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYva\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtr_1h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYva_1h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'to_categorical' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from MKLpy.algorithms import EasyMKL\n",
    "from MKLpy.utils.misc import identity_kernel\n",
    "from MKLpy.preprocessing import normalization\n",
    "import numpy as np\n",
    "\n",
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "data = load_iris()\n",
    "X, Y = data.data, data.target\n",
    "num_classes = len(np.unique(Y))\n",
    "Yh = torch.tensor(to_categorical(Y), dtype=torch.float32)\n",
    "Xtr, Xva, Ytr, Yva, Ytr_1h, Yva_1h = train_test_split(X, Y, Yh, random_state=42, shuffle=True, test_size=.3)\n",
    "\n",
    "\n",
    "learning_rate = 1e-5\n",
    "batch_size    = 32\n",
    "activation    = nn.Sigmoid()\n",
    "num_hidden    = 10\n",
    "num_neurons   = 128\n",
    "max_epochs    = 100\n",
    "\n",
    "train_data = IrisDataset(Xtr, Ytr_1h)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd0083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_hidden, num_neurons, num_classes, activation):\n",
    "        super(MLP, self).__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_classes = num_classes\n",
    "        self.activation = activation\n",
    "        self.layers = nn.ModuleList()\n",
    "        for l in range(1, self.num_hidden+1):\n",
    "            layer = nn.Linear(num_neurons, num_neurons)\n",
    "            self.layers.append(layer)\n",
    "        self.classification_layer = nn.Linear(num_neurons, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.classification_layer(x)\n",
    "        return x\n",
    "\n",
    "model = MLP(num_hidden=num_hidden, num_neurons=num_neurons, num_classes=num_classes, activation=activation)\n",
    "\n",
    "train_data = IrisDataset(Xtr, Ytr_1h)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = IrisDataset(Xva, Yva_1h)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "reduce_lr  = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.2, patience=5, min_lr=0.001)\n",
    "earlystop  = callb.EarlyStopping(\n",
    "    monitor='val_loss',patience=10, mode='min',verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6738a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = nn.MSELoss()(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = nn.MSELoss()(model(torch.tensor(Xva, dtype=torch.float32)), Yva_1h)\n",
    "        reduce_lr.step(val_loss)\n",
    "\n",
    "#representations extraction and kernels definition\n",
    "train_representations = []\n",
    "test_representations = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, _ = data\n",
    "        representations = model.layers[0](inputs)\n",
    "        for i in range(1, len(model.layers)):\n",
    "            representations = model.layers[i](model.activation(representations))\n",
    "        train_representations.append(representations)\n",
    "\n",
    "    for data in test_loader:\n",
    "        inputs, _ = data\n",
    "        representations = model.layers[0](inputs)\n",
    "        for i in range(1, len(model.layers)):\n",
    "            representations = model.layers[i](model.activation(representations))\n",
    "        test_representations.append(representations)\n",
    "\n",
    "train_representations = torch.cat(train_representations).numpy()\n",
    "test_representations = torch.cat(test_representations).numpy()\n",
    "\n",
    "weights = EasyMKL().fit(normalization(train_representations, axis=0), Ytr)\n",
    "kernel = lambda X1, X2: weights.dot(identity_kernel(X1, X2))\n",
    "Ktr = kernel(train_representations, train_representations)\n",
    "Kva = kernel(test_representations, train_representations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10ec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "接下来，我们需要将从MLP的最后一层（也就是输出层）获取的特征表示提取出来，然后将这些特征表示作为EasyMKL算法的输入，得到最终的预测结果。\n",
    "\n",
    "我们可以通过在MLP模型中添加一层来提取特征表示，然后将这些特征表示输入EasyMKL算法。具体地，我们可以在MLP模型的最后一层（输出层）之前添加一层，使其输出的是特征表示而不是最终的预测结果。这一层可以是任何合适的层，例如全连接层或池化层。\n",
    "\n",
    "在本例中，我们可以添加一个全连接层作为特征提取层。具体来说，我们可以在MLP类的构造函数中添加以下代码：\n",
    "\n",
    "self.feature_layer = nn.Linear(num_neurons, num_neurons)\n",
    "\n",
    "#在前向传递函数中，我们在输出层之前将数据传递到特征提取层，并将其结果用于EasyMKL算法。修改后的前向传递函数如下所示：\n",
    "\n",
    "def forward(self, x):\n",
    "    for layer in self.layers:\n",
    "        x = self.activation(layer(x))\n",
    "        features = self.feature_layer(x)\n",
    "        x = self.classification_layer(x)\n",
    "    return x, features\n",
    "\n",
    "#我们还需要修改训练循环，以便我们可以在训练过程中提取特征表示，并使用EasyMKL算法进行预测。具体来说，我们需要在每个训练循环中，使用MLP模型来预测训练数据和测试数据的特征表示，并将这些特征表示传递给EasyMKL算法进行训练和预测。修改后的训练循环如下所示：\n",
    "\n",
    "train_kernel = None\n",
    "test_kernel = None\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output, features = model(data)\n",
    "        loss = nn.MSELoss()(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute kernel matrices\n",
    "        if train_kernel is None:\n",
    "            train_kernel = identity_kernel(features.detach().numpy())\n",
    "        else:\n",
    "            train_kernel += identity_kernel(features.detach().numpy())\n",
    "\n",
    "    # Compute kernel matrices for test set\n",
    "    test_features = model(torch.tensor(Xte, dtype=torch.float32))[1].detach().numpy()\n",
    "    if test_kernel is None:\n",
    "        test_kernel = identity_kernel(test_features)\n",
    "    else:\n",
    "        test_kernel += identity_kernel(test_features)\n",
    "    #Normalize kernel matrices\n",
    "    train_kernel = normalization(train_kernel)\n",
    "    test_kernel = normalization(test_kernel)\n",
    "\n",
    "    #Train and predict using EasyMKL\n",
    "    clf = EasyMKL(lam=0.1)\n",
    "    clf.fit(train_kernel, Ytr)\n",
    "    Yte_pred = clf.predict(test_kernel)\n",
    "\n",
    "最终的预测结果存储在Yte_pred中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db98cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                  *  D:\\Software\\Anaconda\n",
      "dlenv                    D:\\Software\\Anaconda\\envs\\dlenv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda info -e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
